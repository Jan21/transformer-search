{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoModelForCausalLM\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from utils.data import get_data_for_inference, get_tokenizer, Datamodule\n",
    "\n",
    "def load_model_and_tokenizer(config):\n",
    "    print(f\"Loading model from {config.inference.notebook_modelpath}...\")\n",
    "    \n",
    "    # Load tokenizer using the same function from your original code\n",
    "    tokenizer = get_tokenizer(config.tok_data)\n",
    "    \n",
    "    # Load model\n",
    "    model_dir = Path(config.inference.notebook_modelpath)\n",
    "    \n",
    "    # Check if we have a state dict saved separately\n",
    "    state_dict_path = model_dir / \"model.pth\"\n",
    "    state_dict = None\n",
    "    if state_dict_path.exists():\n",
    "        print(\"Loading state dict from model.pth...\")\n",
    "        state_dict = torch.load(state_dict_path)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        local_files_only=True,\n",
    "        state_dict=state_dict,\n",
    "        # Use flash attention if available\n",
    "        attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_single_example(config, tokenizer, sample_id):\n",
    "    datapath = config.tok_data.val_file\n",
    "    print(f\"Loading example from {datapath}...\")\n",
    "    \n",
    "    # Load the example from JSON\n",
    "    with open(datapath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the text directly from the first item\n",
    "    example_text = data[sample_id][\"text\"]\n",
    "    \n",
    "    # Find if there's a \"Command:\" delimiter in the prompt\n",
    "    delimiter = config.data.split_str\n",
    "\n",
    "    parts = example_text.split(delimiter, 1)\n",
    "    prompt = parts[0] + delimiter\n",
    "    \n",
    "    # Store the ground truth (what comes after \"Command:\") for comparison\n",
    "    ground_truth = parts[1].strip()\n",
    "    print(f\"Ground truth answer: '{ground_truth}'\")\n",
    "    \n",
    "    print(f\"Loaded example prompt: {prompt[:100]}...\")  # Print first 100 chars\n",
    "    \n",
    "    if tokenizer.bos_token and not prompt.startswith(tokenizer.bos_token):\n",
    "        full_prompt = tokenizer.bos_token + \" \" + prompt\n",
    "    else:\n",
    "        full_prompt = prompt\n",
    "    \n",
    "    return full_prompt, {\"text\": example_text, \"ground_truth\": ground_truth}\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, config):\n",
    "    print(\"Tokenizing prompt and running inference...\")\n",
    "    \n",
    "    # Get device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    prompt_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Generate outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs.get(\"attention_mask\", None),\n",
    "            max_length=config.model.block_size,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            num_beams=1,  # Greedy decoding\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Get the generated text (everything after the prompt)\n",
    "    generated_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=False)\n",
    "    \n",
    "    # Get token-level output\n",
    "    generated_ids = outputs[0][prompt_length:].tolist()\n",
    "    \n",
    "    result = generated_text.strip()\n",
    "    \n",
    "    return result, generated_text, generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('config/config_base.yaml')\n",
    "\n",
    "# This will trigger Hydra configuration loading\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(cfg)\n",
    "\n",
    "# Load the example\n",
    "prompt, example = load_single_example(cfg, tokenizer, sample_id=42)\n",
    "\n",
    "# Run inference\n",
    "result, full_generated_text, generated_ids = run_inference(model, tokenizer, prompt, cfg)\n",
    "print(\"\\nexample:\", example, \"\\n\")\n",
    "print(\"result:\", result, \"\\n\",\n",
    "        \"full_generated_text:\", \n",
    "        full_generated_text, \"\\n\", \n",
    "        \"generated_ids:\", \n",
    "        generated_ids)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
