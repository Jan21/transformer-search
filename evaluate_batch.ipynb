{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0392a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM\n",
    "from omegaconf import DictConfig\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from utils.data import get_tokenizer  \n",
    "\n",
    "###############################################################################\n",
    "#EVALUATE ONE EXAMPLE\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# 1) Hardcode the data string\n",
    "###############################################################################\n",
    "HARD_CODED_STRING = (\n",
    "    \"Init_state: [ 0 : 6 , 1 : 5 , 2 : 3 , 3 : 1 , 4 : 0 ] Stack: [  { 0 : 2 , 1 : 6 , 2 : 3 , 3 : 4 , 4 : 5 } ]\"\n",
    ")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Define utility functions for parsing / re-building the string\n",
    "###############################################################################\n",
    "def parse_data_string(data_str: str):\n",
    "    \"\"\"\n",
    "    Parse a string of the form:\n",
    "      Init_state: [ 0 : 6 , 1 : 5 , ... ] Stack: [  { 0 : 2 } , { 1 : 3 , 2 : 5 } ]\n",
    "    into:\n",
    "      init_state = {0: 6, 1: 5, ...}\n",
    "      stack = [ {0: 2}, {1: 3, 2: 5}, ...]\n",
    "    \"\"\"\n",
    "    # --- Extract the init_state portion\n",
    "    init_pat = r\"Init_state:\\s*\\[(.*?)\\]\"\n",
    "    init_match = re.search(init_pat, data_str)\n",
    "    if not init_match:\n",
    "        raise ValueError(\"Could not find 'Init_state: [...]' in string.\")\n",
    "    init_content = init_match.group(1).strip()\n",
    "\n",
    "    # --- Extract the stack portion\n",
    "    stack_pat = r\"Stack:\\s*\\[(.*?)\\]$\"\n",
    "    stack_match = re.search(stack_pat, data_str)\n",
    "    if not stack_match:\n",
    "        raise ValueError(\"Could not find 'Stack: [...]' in string.\")\n",
    "    stack_content = stack_match.group(1).strip()\n",
    "\n",
    "    # --- Parse init_state into a dict\n",
    "    init_state = {}\n",
    "    for pair_str in init_content.split(\",\"):\n",
    "        pair_str = pair_str.strip()\n",
    "        if not pair_str:\n",
    "            continue\n",
    "        k_str, v_str = pair_str.split(\":\")\n",
    "        k = int(k_str.strip())\n",
    "        v = int(v_str.strip())\n",
    "        init_state[k] = v\n",
    "\n",
    "    # --- Parse the stack (list of dicts)\n",
    "    raw_dicts = re.findall(r\"\\{(.*?)\\}\", stack_content)\n",
    "    stack_list = []\n",
    "    for rd in raw_dicts:\n",
    "        d = {}\n",
    "        pairs = rd.split(\",\")\n",
    "        for p in pairs:\n",
    "            p = p.strip()\n",
    "            if not p:\n",
    "                continue\n",
    "            k_str, v_str = p.split(\":\")\n",
    "            d[int(k_str.strip())] = int(v_str.strip())\n",
    "        stack_list.append(d)\n",
    "\n",
    "    return init_state, stack_list\n",
    "\n",
    "\n",
    "def data_to_string(init_state: Dict[int,int], stack_list: List[Dict[int,int]]) -> str:\n",
    "    \"\"\"\n",
    "    Convert init_state + stack_list back to the same string format:\n",
    "      \"Init_state: [ 0 : 6 , ... ] Stack: [  { 0 : 2 , ...} , { ... } ]\"\n",
    "    \"\"\"\n",
    "    # Build init_state string\n",
    "    init_entries = [f\"{k} : {v}\" for k, v in init_state.items()]\n",
    "    init_str = \" , \".join(init_entries)\n",
    "\n",
    "    # Build stack string\n",
    "    stack_strs = []\n",
    "    for d in stack_list:\n",
    "        pairs = [f\"{k} : {v}\" for k, v in d.items()]\n",
    "        dict_str = \"{ \" + \" , \".join(pairs) + \" }\"\n",
    "        stack_strs.append(dict_str)\n",
    "    total_stack_str = \" , \".join(stack_strs)\n",
    "\n",
    "    return f\"Init_state: [ {init_str} ] Stack: [  {total_stack_str} ]\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) Cleanup + Command Application\n",
    "###############################################################################\n",
    "def cleanup_stack(stack: List[Dict[int,int]]):\n",
    "    \"\"\"Remove any empty dictionaries from the beginning of the stack.\"\"\"\n",
    "    while stack and not stack[0]:\n",
    "        stack.pop(0)\n",
    "\n",
    "def apply_command(op: str, init_state: Dict[int,int], stack: List[Dict[int,int]]):\n",
    "    \"\"\"\n",
    "    Apply a command to init_state & stack.\n",
    "    This includes:\n",
    "     - Movement commands: \"X D\" or \"X U\"\n",
    "     - TF, RL, N {...} stubs (you can adapt your own rules here).\n",
    "    \"\"\"\n",
    "\n",
    "    # Movement commands: \"X D\" or \"X U\"\n",
    "    if op.endswith(\" D\") or op.endswith(\" U\"):\n",
    "        parts = op.split()\n",
    "        if len(parts) != 2:\n",
    "            print(f\"Error: Malformed movement command '{op}'\")\n",
    "            return\n",
    "        automat_index, direction = parts\n",
    "        automat_index = int(automat_index)\n",
    "        if direction == \"D\":\n",
    "            if init_state[automat_index] > 0:\n",
    "                init_state[automat_index] -= 1\n",
    "            else:\n",
    "                print(f\"Warning: Automat {automat_index} is already at 0.\")\n",
    "        elif direction == \"U\":\n",
    "            if init_state[automat_index] < 6:\n",
    "                init_state[automat_index] += 1\n",
    "            else:\n",
    "                print(f\"Warning: Automat {automat_index} is already at max 6.\")\n",
    "        return\n",
    "\n",
    "    # TF (Take First)\n",
    "    if op == \"TF\":\n",
    "        cleanup_stack(stack)\n",
    "        if not stack or not stack[0]:\n",
    "            print(\"Warning: TF on empty stack or empty first dict.\")\n",
    "            return\n",
    "        first_dict = stack[0]\n",
    "        # pop the first key-value\n",
    "        key = next(iter(first_dict))\n",
    "        value = first_dict.pop(key)\n",
    "        from collections import OrderedDict\n",
    "        new_dict = OrderedDict([(key, value)])\n",
    "        # Insert the new OrderedDict at the beginning\n",
    "        stack.insert(0, new_dict)\n",
    "        return\n",
    "\n",
    "    # RL (Remove if matches init_state)\n",
    "    if op == \"RL\":\n",
    "        cleanup_stack(stack)\n",
    "        if not stack:\n",
    "            print(\"Warning: RL on empty stack.\")\n",
    "            return\n",
    "        first_dict = stack[0]\n",
    "        valid = True\n",
    "        for k, v in first_dict.items():\n",
    "            if init_state.get(k) != v:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            stack.pop(0)\n",
    "        else:\n",
    "            print(\"Warning: RL mismatch between stack[0] and init_state.\")\n",
    "        return\n",
    "\n",
    "    # N {...} (Add new dict at the beginning of the stack)\n",
    "    if op.startswith(\"N \"):\n",
    "        dict_str = op[2:].strip()  # everything after \"N \"\n",
    "        from collections import OrderedDict\n",
    "        try:\n",
    "            new_pair = ast.literal_eval(dict_str)\n",
    "            if not isinstance(new_pair, dict) or not (1 <= len(new_pair) <= 2):\n",
    "                print(\"Warning: N operation requires 1-2 key-value pairs.\")\n",
    "                return\n",
    "            cleanup_stack(stack)\n",
    "            # Insert a new OrderedDict at the beginning\n",
    "            stack.insert(0, OrderedDict(new_pair.items()))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: error parsing 'N' dictionary: {e}\")\n",
    "        return\n",
    "\n",
    "    # If we get here, we have an unknown operation\n",
    "    print(f\"Warning: Unknown operation '{op}'.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) Load Model & Tokenizer (uses get_tokenizer from your snippet)\n",
    "###############################################################################\n",
    "def load_model_and_tokenizer(config) -> (AutoModelForCausalLM, Any):\n",
    "    \"\"\"\n",
    "    Exactly as you specified:\n",
    "        def load_model_and_tokenizer(config):\n",
    "            print(f\"Loading model from {config.inference.notebook_modelpath}...\")\n",
    "            tokenizer = get_tokenizer(config.tok_data)\n",
    "            model_dir = Path(config.inference.notebook_modelpath)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_dir,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            return model, tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {config.inference.notebook_modelpath}...\")\n",
    "\n",
    "    # Load tokenizer using the same function from your original code\n",
    "    tokenizer = get_tokenizer(config.tok_data)\n",
    "\n",
    "    # Load model\n",
    "    model_dir = Path(config.inference.notebook_modelpath)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Generate a command from the current data\n",
    "###############################################################################\n",
    "def generate_command(model, tokenizer, prompt: str, max_length=128) -> str:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Remove token_type_ids (if present) before generating\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=prompt_len + max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=1,  # greedy\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    generated = full_text[len(prompt):].strip()\n",
    "    return generated\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) Inference Loop\n",
    "###############################################################################\n",
    "def iterative_inference_loop(model, tokenizer, config, max_iters):\n",
    "    \"\"\"\n",
    "    - Parse the HARD_CODED_STRING\n",
    "    - Repeatedly prompt the model with the current data\n",
    "    - Apply the command\n",
    "    - Stop if init_state == original_stack[0] or we hit max_iters\n",
    "    \"\"\"\n",
    "\n",
    "    # 6.1 Parse the original data\n",
    "    original_init_state, original_stack = parse_data_string(HARD_CODED_STRING)\n",
    "    if not original_stack:\n",
    "        print(\"Error: Original stack is empty. Cannot continue.\")\n",
    "        return\n",
    "\n",
    "    # The first dictionary in the original stack is our \"target\" for termination\n",
    "    target_dict = original_stack[0].copy()\n",
    "\n",
    "    # 6.2 Create current copies\n",
    "    current_init_state = original_init_state.copy()\n",
    "    current_stack = [d.copy() for d in original_stack]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        # Convert current data to string\n",
    "        current_data_str = data_to_string(current_init_state, current_stack)\n",
    "\n",
    "        # Build prompt. \n",
    "        # If your tokenizer has a bos_token, prepend it. If not, just use the raw.\n",
    "        bos_token = getattr(tokenizer, \"bos_token\", \"\")\n",
    "        prompt = f\"{bos_token} {current_data_str} Command:\"\n",
    "\n",
    "        # 6.3 Generate the command\n",
    "        command_pred = generate_command(model, tokenizer, prompt)\n",
    "        # e.g. could be \"0 D [EOS]\" or \"1 U\"\n",
    "        command_pred = command_pred.replace(\"[EOS]\", \"\").strip()\n",
    "\n",
    "        print(f\"\\nIteration={i}, current_data='{current_data_str}'\")\n",
    "        print(f\"Model predicted command: '{command_pred}'\")\n",
    "\n",
    "        # 6.4 Apply the command\n",
    "        apply_command(command_pred, current_init_state, current_stack)\n",
    "\n",
    "        # 6.5 Check termination condition\n",
    "        if current_init_state == target_dict:\n",
    "            print(\"Termination: 'init_state' matches the first dictionary of the original stack!\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Reached {max_iters} iterations without matching target dict.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60168f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "def main(config_path: str = \"config/config_base.yaml\"):\n",
    "    \"\"\"\n",
    "    Loads config from 'config/config_base.yaml',\n",
    "    then loads model/tokenizer, and performs the iterative inference loop.\n",
    "    \"\"\"\n",
    "    # 1) Load config\n",
    "    cfg = OmegaConf.load(config_path)\n",
    "\n",
    "    # 2) Load model + tokenizer (defined in Cell #1)\n",
    "    model, tokenizer = load_model_and_tokenizer(cfg)\n",
    "\n",
    "    # 3) Run iterative inference (defined in Cell #1)\n",
    "    iterative_inference_loop(model, tokenizer, cfg, max_iters=200)\n",
    "\n",
    "# Actually run it\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ab19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from utils.data import get_tokenizer  \n",
    "\n",
    "###############################################################################\n",
    "#EVALUATE FROM JSON\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# 1) Define utility functions for parsing / re-building the string\n",
    "###############################################################################\n",
    "def parse_data_string(data_str: str):\n",
    "    \"\"\"\n",
    "    Parse a string of the form:\n",
    "      Init_state: [ 0 : 6 , 1 : 5 , ... ] Stack: [  { 0 : 2 } , { 1 : 3 , 2 : 5 } ]\n",
    "    into:\n",
    "      init_state = {0: 6, 1: 5, ...}\n",
    "      stack = [ {0: 2}, {1: 3, 2: 5}, ...]\n",
    "    \"\"\"\n",
    "    # --- Extract the init_state portion\n",
    "    init_pat = r\"Init_state:\\s*\\[(.*?)\\]\"\n",
    "    init_match = re.search(init_pat, data_str)\n",
    "    if not init_match:\n",
    "        raise ValueError(\"Could not find 'Init_state: [...]' in string.\")\n",
    "    init_content = init_match.group(1).strip()\n",
    "\n",
    "    # --- Extract the stack portion\n",
    "    stack_pat = r\"Stack:\\s*\\[(.*?)\\]$\"\n",
    "    stack_match = re.search(stack_pat, data_str)\n",
    "    if not stack_match:\n",
    "        raise ValueError(\"Could not find 'Stack: [...]' in string.\")\n",
    "    stack_content = stack_match.group(1).strip()\n",
    "\n",
    "    # --- Parse init_state into a dict\n",
    "    init_state = {}\n",
    "    for pair_str in init_content.split(\",\"):\n",
    "        pair_str = pair_str.strip()\n",
    "        if not pair_str:\n",
    "            continue\n",
    "        k_str, v_str = pair_str.split(\":\")\n",
    "        k = int(k_str.strip())\n",
    "        v = int(v_str.strip())\n",
    "        init_state[k] = v\n",
    "\n",
    "    # --- Parse the stack (list of dicts)\n",
    "    raw_dicts = re.findall(r\"\\{(.*?)\\}\", stack_content)\n",
    "    stack_list = []\n",
    "    for rd in raw_dicts:\n",
    "        d = {}\n",
    "        pairs = rd.split(\",\")\n",
    "        for p in pairs:\n",
    "            p = p.strip()\n",
    "            if not p:\n",
    "                continue\n",
    "            k_str, v_str = p.split(\":\")\n",
    "            d[int(k_str.strip())] = int(v_str.strip())\n",
    "        stack_list.append(d)\n",
    "\n",
    "    return init_state, stack_list\n",
    "\n",
    "\n",
    "def data_to_string(init_state: Dict[int,int], stack_list: List[Dict[int,int]]) -> str:\n",
    "    \"\"\"\n",
    "    Convert init_state + stack_list back to the same string format:\n",
    "      \"Init_state: [ 0 : 6 , ... ] Stack: [  { 0 : 2 , ...} , { ... } ]\"\n",
    "    \"\"\"\n",
    "    # Build init_state string\n",
    "    init_entries = [f\"{k} : {v}\" for k, v in init_state.items()]\n",
    "    init_str = \" , \".join(init_entries)\n",
    "\n",
    "    # Build stack string\n",
    "    stack_strs = []\n",
    "    for d in stack_list:\n",
    "        pairs = [f\"{k} : {v}\" for k, v in d.items()]\n",
    "        dict_str = \"{ \" + \" , \".join(pairs) + \" }\"\n",
    "        stack_strs.append(dict_str)\n",
    "    total_stack_str = \" , \".join(stack_strs)\n",
    "\n",
    "    return f\"Init_state: [ {init_str} ] Stack: [  {total_stack_str} ]\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) Cleanup + Command Application\n",
    "###############################################################################\n",
    "def cleanup_stack(stack: List[Dict[int,int]]):\n",
    "    \"\"\"Remove any empty dictionaries from the beginning of the stack.\"\"\"\n",
    "    while stack and not stack[0]:\n",
    "        stack.pop(0)\n",
    "\n",
    "def apply_command(op: str, init_state: Dict[int,int], stack: List[Dict[int,int]]):\n",
    "    \"\"\"\n",
    "    Apply a command to init_state & stack.\n",
    "    This includes:\n",
    "     - Movement commands: \"X D\" or \"X U\"\n",
    "     - TF, RL, N {...} stubs (you can adapt your own rules here).\n",
    "    \"\"\"\n",
    "\n",
    "    # Movement commands: \"X D\" or \"X U\"\n",
    "    if op.endswith(\" D\") or op.endswith(\" U\"):\n",
    "        parts = op.split()\n",
    "        if len(parts) != 2:\n",
    "            print(f\"Error: Malformed movement command '{op}'\")\n",
    "            return\n",
    "        automat_index, direction = parts\n",
    "        automat_index = int(automat_index)\n",
    "        if direction == \"D\":\n",
    "            if init_state[automat_index] > 0:\n",
    "                init_state[automat_index] -= 1\n",
    "            else:\n",
    "                print(f\"Warning: Automat {automat_index} is already at 0.\")\n",
    "        elif direction == \"U\":\n",
    "            if init_state[automat_index] < 6:\n",
    "                init_state[automat_index] += 1\n",
    "            else:\n",
    "                print(f\"Warning: Automat {automat_index} is already at max 6.\")\n",
    "        return\n",
    "\n",
    "    # TF (Take First)\n",
    "    if op == \"TF\":\n",
    "        cleanup_stack(stack)\n",
    "        if not stack or not stack[0]:\n",
    "            print(\"Warning: TF on empty stack or empty first dict.\")\n",
    "            return\n",
    "        first_dict = stack[0]\n",
    "        # pop the first key-value\n",
    "        key = next(iter(first_dict))\n",
    "        value = first_dict.pop(key)\n",
    "        from collections import OrderedDict\n",
    "        new_dict = OrderedDict([(key, value)])\n",
    "        # Insert the new OrderedDict at the beginning\n",
    "        stack.insert(0, new_dict)\n",
    "        return\n",
    "\n",
    "    # RL (Remove if matches init_state)\n",
    "    if op == \"RL\":\n",
    "        cleanup_stack(stack)\n",
    "        if not stack:\n",
    "            print(\"Warning: RL on empty stack.\")\n",
    "            return\n",
    "        first_dict = stack[0]\n",
    "        valid = True\n",
    "        for k, v in first_dict.items():\n",
    "            if init_state.get(k) != v:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            stack.pop(0)\n",
    "        else:\n",
    "            print(\"Warning: RL mismatch between stack[0] and init_state.\")\n",
    "        return\n",
    "\n",
    "    # N {...} (Add new dict at the beginning of the stack)\n",
    "    if op.startswith(\"N \"):\n",
    "        dict_str = op[2:].strip()  # everything after \"N \"\n",
    "        from collections import OrderedDict\n",
    "        try:\n",
    "            new_pair = ast.literal_eval(dict_str)\n",
    "            if not isinstance(new_pair, dict) or not (1 <= len(new_pair) <= 2):\n",
    "                print(\"Warning: N operation requires 1-2 key-value pairs.\")\n",
    "                return\n",
    "            cleanup_stack(stack)\n",
    "            # Insert a new OrderedDict at the beginning\n",
    "            stack.insert(0, OrderedDict(new_pair.items()))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: error parsing 'N' dictionary: {e}\")\n",
    "        return\n",
    "\n",
    "    # If we get here, we have an unknown operation\n",
    "    print(f\"Warning: Unknown operation '{op}'.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) Load Model & Tokenizer (uses get_tokenizer from your snippet)\n",
    "###############################################################################\n",
    "def load_model_and_tokenizer(config) -> (AutoModelForCausalLM, Any):\n",
    "    \"\"\"\n",
    "    Exactly as you specified:\n",
    "        def load_model_and_tokenizer(config):\n",
    "            print(f\"Loading model from {config.inference.notebook_modelpath}...\")\n",
    "            tokenizer = get_tokenizer(config.tok_data)\n",
    "            model_dir = Path(config.inference.notebook_modelpath)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_dir,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            return model, tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {config.inference.notebook_modelpath}...\")\n",
    "\n",
    "    # Load tokenizer using the same function from your original code\n",
    "    tokenizer = get_tokenizer(config.tok_data)\n",
    "\n",
    "    # Load model\n",
    "    model_dir = Path(config.inference.notebook_modelpath)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) Generate a command from the current data\n",
    "###############################################################################\n",
    "def generate_command(model, tokenizer, prompt: str, max_length=128) -> str:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Remove token_type_ids (if present) before generating\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=prompt_len + max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=1,  # greedy\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    generated = full_text[len(prompt):].strip()\n",
    "    return generated\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Inference Loop evaluation\n",
    "###############################################################################\n",
    "def evaluate_example(data_str: str,\n",
    "                     model: AutoModelForCausalLM,\n",
    "                     tokenizer: Any,\n",
    "                     max_iters: int = 200) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the loop terminates by matching init_state to the target_dict\n",
    "    within max_iters, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        original_init_state, original_stack = parse_data_string(data_str)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] parsing example: {e}\")\n",
    "        return False\n",
    "\n",
    "    if not original_stack:\n",
    "        print(\"[ERROR] example has empty original stack\")\n",
    "        return False\n",
    "\n",
    "    target_dict = original_stack[0].copy()\n",
    "    current_init_state = original_init_state.copy()\n",
    "    current_stack = [d.copy() for d in original_stack]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        current_data_str = data_to_string(current_init_state, current_stack)\n",
    "        bos = getattr(tokenizer, \"bos_token\", \"\")\n",
    "        prompt = f\"{bos} {current_data_str} Command:\"\n",
    "        command_pred = generate_command(model, tokenizer, prompt)\n",
    "        command_pred = command_pred.replace(\"[EOS]\", \"\").strip()\n",
    "        apply_command(command_pred, current_init_state, current_stack)\n",
    "        if current_init_state == target_dict:\n",
    "            return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eda52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os    \n",
    "\n",
    "def main(config_path: str = \"config/config_base.yaml\"):\n",
    "    # --- Load config, model, tokenizer as before ---\n",
    "    cfg = OmegaConf.load(config_path)\n",
    "    model, tokenizer = load_model_and_tokenizer(cfg)\n",
    "\n",
    "    # --- Load JSON of examples ---\n",
    "    # *** ADDED: specify your JSON file here ***\n",
    "    json_file = \"data/test_only_first.json\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "        examples = json.load(jf)\n",
    "\n",
    "    passed, failed = 0, 0\n",
    "\n",
    "    # --- Iterate through each example and evaluate ---\n",
    "    for idx, ex in enumerate(examples):\n",
    "        text = ex.get(\"text\", \"\")\n",
    "        result = evaluate_example(text, model, tokenizer, max_iters=130)\n",
    "        if result:\n",
    "            passed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "        print(f\"Example {idx+1}/{len(examples)}: {'PASSED' if result else 'FAILED'}\")\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(f\"\\nTotal examples: {len(examples)}\")\n",
    "    print(f\"✔ Passed: {passed}\")\n",
    "    print(f\"✘ Failed: {failed}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
