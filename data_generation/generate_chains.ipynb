{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n chains of length m, where n and m are given by the user, default n = 5, m = 10\n",
    "# fix some order of the chains\n",
    "# add dependencies on the edges such that the edge of one chain depends on the states of later chains (not the previous ones in the fixed order)\n",
    "# the number of states on which a given edge depends is chosen uniformly from {0,1, 2, 3}..if there are multiple states, they need to be from different chains.\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def gen_chains(n=5, m=7):\n",
    "    chains = defaultdict(list)\n",
    "\n",
    "    for i in range(n):\n",
    "        G = nx.Graph()\n",
    "        nodes = [j for j in range(m)]\n",
    "        edges = list(zip(nodes[:-1], nodes[1:]))\n",
    "\n",
    "        G.add_edges_from(edges)\n",
    "        chains[i] = G\n",
    "\n",
    "    return chains\n",
    "\n",
    "def add_enabling(chains):\n",
    "    n = len(chains)\n",
    "    \n",
    "    for chain_id in range(n-1):\n",
    "        G = chains[chain_id]\n",
    "        \n",
    "        for edge in G.edges():\n",
    "            if random.random() < 0.5:\n",
    "                num_deps = random.randint(1, 3)\n",
    "                \n",
    "                if num_deps > 0:\n",
    "                    later_chains = list(range(chain_id + 1, n))\n",
    "                    selected_chains = random.sample(later_chains, min(num_deps, len(later_chains)))\n",
    "                    \n",
    "                    enabling_conditions = []\n",
    "                    for enabling_chain in selected_chains:\n",
    "                        enabling_G = chains[enabling_chain]\n",
    "                        enabling_node = random.choice(list(enabling_G.nodes()))\n",
    "                        enabling_conditions.append({'automata_id':enabling_chain,'node_id':enabling_node})\n",
    "                    tmp = [enabling_conditions, [{'automata_id':n-1,'node_id':9999}]]\n",
    "                    \n",
    "                    G.edges[edge]['enabling'] = tmp\n",
    "    \n",
    "    return chains\n",
    "\n",
    "chains = add_enabling(gen_chains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chain_id in range(len(chains)):\n",
    "    print(f\"\\nChain {chain_id}:\")\n",
    "    G = chains[chain_id]\n",
    "    for edge in G.edges(data=True):\n",
    "        print(f\"Edge {edge[0]} -> {edge[1]}: {edge[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(vector, graphs):\n",
    "    num_states = len(graphs[0].nodes())\n",
    "    vec = list(vector)  # [1,2,3,4]\n",
    "    for i in range(num_states):\n",
    "        if vec[0] != i:\n",
    "            yield vec, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def get_vectors(chains):\n",
    "    states_per_chain = []\n",
    "    for chain_id in range(len(chains)):\n",
    "        states_per_chain.append(list(chains[chain_id].nodes()))\n",
    "    \n",
    "    return list(product(*states_per_chain))\n",
    "\n",
    "vecs = get_vectors(chains)\n",
    "print(len(vecs))\n",
    "print(vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "id_2_vec = {}\n",
    "vec_2_id = {}\n",
    "for i, comb in tqdm(enumerate(vecs)):\n",
    "    id_2_vec[i] = comb\n",
    "    vec_2_id[comb] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(len(vecs)))\n",
    "samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unique_paths(paths_lst):\n",
    "    if not paths_lst:\n",
    "        return []\n",
    "    \n",
    "    # Keep track of which paths should be excluded\n",
    "    paths_to_exclude = set()\n",
    "    \n",
    "    # For each path, check if it contains any shorter path as a subsequence\n",
    "    for path in paths_lst:\n",
    "        for other_path in paths_lst:\n",
    "            # Skip if same path or if other_path is not shorter\n",
    "            if path == other_path or len(other_path) >= len(path):\n",
    "                continue\n",
    "                \n",
    "            # Check if other_path is a subsequence of path\n",
    "            j, k = 0, 0  # j for other_path, k for path\n",
    "            while j < len(other_path) and k < len(path):\n",
    "                if other_path[j] == path[k]:\n",
    "                    j += 1\n",
    "                k += 1\n",
    "                \n",
    "            # If other_path is a subsequence of path, exclude path\n",
    "            if j == len(other_path):\n",
    "                paths_to_exclude.add(tuple(path))\n",
    "                break\n",
    "    \n",
    "    # Return all paths except those that contain shorter paths as subsequences\n",
    "    return [path for path in paths_lst if tuple(path) not in paths_to_exclude]\n",
    "\n",
    "def solve(curr_states, target_node, graphs, orig_state_len, log=[], depth=0):\n",
    "    '''\n",
    "    Solves the problem of reaching target node from current state in graphs by returning the resulting state and log of steps.\n",
    "    Parameters:\n",
    "        curr_states: list of current states of automatas; LIST of INTEGERS\n",
    "        target_node: target node; INTEGER\n",
    "        graphs: list of graphs; LIST of networkx graphs\n",
    "        log: list of log messages; LIST of STRINGS\n",
    "        depth: depth of recursion; INTEGER\n",
    "    Returns:\n",
    "        out: states of automatas after solving or None if no solution; LIST of INTEGERS/None\n",
    "            out: log: list of log messages; LIST of STRINGS\n",
    "    '''\n",
    "    indent = '-'*depth*2\n",
    "    # log.append(f\"{'-'*50}\")\n",
    "    curr_states_cpy = curr_states.copy()\n",
    "    log.append(f\"{indent}S {curr_states_cpy} TN {target_node}\")\n",
    "    # get all paths from current state to target node\n",
    "    paths_lst = list(nx.all_simple_paths(graphs[0], curr_states_cpy[0], target_node))\n",
    "    # check if a shorter path isn't already in a different longer path\n",
    "    paths_lst = filter_unique_paths(paths_lst)\n",
    "    log.append(f\"{indent}PL {paths_lst}\")\n",
    "    \n",
    "    # check if there is a path\n",
    "    if len(paths_lst) == 0:\n",
    "        log.append(f\"{indent}NOPATH {curr_states} TN {target_node}\")\n",
    "        return None, log\n",
    "    \n",
    "    # check if current node equals target node\n",
    "    if curr_states_cpy[0] == target_node:\n",
    "        log.append(f\"{indent}REND {curr_states_cpy}\")\n",
    "        return curr_states_cpy, log\n",
    "    \n",
    "    # path or paths exist and we are not in target node\n",
    "    for path in paths_lst:\n",
    "        log.append(f\"{indent}P {path}\")\n",
    "        # projdu hrany po ceste (3-4), ziskam enabling\n",
    "        curr_states_cpy = curr_states.copy()\n",
    "        rules = graphs[0].get_edge_data(curr_states_cpy[0], path[1])\n",
    "        # pokud neexistuje enabling, projdu edge\n",
    "        log.append(f\"{indent}RLS {rules}\")\n",
    "        if rules == {}:\n",
    "            log.append(f\"{indent}NOR {curr_states_cpy} TN {target_node}\")\n",
    "            # pokud mÃ¡ path jeden node, tak je to konec\n",
    "            log.append(f\"{indent}{orig_state_len - len(curr_states_cpy)} {'U' if path[1] > curr_states_cpy[0] else 'D'}\")\n",
    "            curr_states_cpy[0] = path[1]\n",
    "            log.append(f\"{indent}NORCH {curr_states_cpy}\")\n",
    "\n",
    "            return solve(curr_states_cpy, target_node, graphs, orig_state_len, log, depth+1)\n",
    "\n",
    "        # pokud existuje enabling, tak treba automat 1 ma byt ve stavu 2 a automat 2 ma byt ve stavu 3\n",
    "        # prvni vyresime prvni pravidlo a pak druhe\n",
    "        else:\n",
    "            for rule_set in rules['enabling']:\n",
    "                log.append(f\"{indent}RS {rule_set}\")\n",
    "\n",
    "                curr_states_cpy = curr_states.copy()\n",
    "                found = True\n",
    "\n",
    "                for rule in sorted(rule_set, key=lambda x: x['automata_id']): # sorted znamena setrizene od nejvice zavisleho po nejmene\n",
    "                    log.append(f\"{indent}RULE {rule}\")\n",
    "                    idx = max(rule['automata_id'] - (orig_state_len - len(curr_states_cpy)), 0)\n",
    "                    current_sub_state, log = solve(curr_states_cpy[idx:], rule['node_id'], graphs[idx:], orig_state_len, log, depth+1)\n",
    "\n",
    "                    log.append(f\"{indent}RCH {current_sub_state}\")\n",
    "                    if current_sub_state == None:\n",
    "                        found = False\n",
    "                        break\n",
    "                    log.append(f\"{indent}SUB {curr_states_cpy}\")\n",
    "                    curr_states_cpy[idx:] = current_sub_state\n",
    "                    log.append(f\"{indent}SUBCH {curr_states_cpy}\")\n",
    "\n",
    "                if found:\n",
    "                    log.append(f\"{indent}SUBF {curr_states_cpy}\")\n",
    "                    log.append(f\"{indent}{orig_state_len - len(curr_states_cpy)} {'U' if path[1] > curr_states_cpy[0] else 'D'}\")\n",
    "                    curr_states_cpy[0] = path[1]\n",
    "                    log.append(f\"{indent}SUBSOLVED {curr_states_cpy}\")\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            if found:\n",
    "                return solve(curr_states_cpy, target_node, graphs, orig_state_len, log, depth+1)\n",
    "            else:\n",
    "                continue\n",
    "    log.append(f\"{indent}UNSOL {curr_states_cpy}\")            \n",
    "    return None, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_graphs(graphs, states, target_nodes, orig_states_len, log=[]):\n",
    "    log_cpy = log.copy()\n",
    "    log_cpy.append(\"*\"*30)\n",
    "    states, log_cpy = solve(states, target_nodes[-1], graphs, orig_states_len, log=log_cpy)\n",
    "\n",
    "    if len(graphs) == 1:\n",
    "        return states, target_nodes, log_cpy\n",
    "\n",
    "    if states is not None:\n",
    "        log = log_cpy\n",
    "        found = False\n",
    "        rnd_target_nodes = [tn for tn in graphs[0].nodes() if tn != states[1]]\n",
    "        random.shuffle(rnd_target_nodes)\n",
    "        for i in rnd_target_nodes:\n",
    "            target_nodes_cpy = target_nodes.copy()\n",
    "            target_nodes_cpy.append(i)\n",
    "            state_tmp, target_nodes_cpy, log_tmp = process_graphs(graphs[1:], states[1:], target_nodes_cpy, orig_states_len, log)\n",
    "            if state_tmp is None:\n",
    "                continue\n",
    "            else:\n",
    "                found = True\n",
    "                log = log_tmp\n",
    "                target_nodes = target_nodes_cpy\n",
    "                states[1:] = state_tmp\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            states = None\n",
    "\n",
    "    return states, target_nodes, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "\n",
    "error_samples = []\n",
    "logs = []\n",
    "for i in tqdm(indexes):\n",
    "    for sample in get_sample(vecs[i], chains):\n",
    "        total += 1\n",
    "\n",
    "        in_states = sample[0]\n",
    "        in_target_nodes = [sample[1]]\n",
    "\n",
    "        states, target_nodes, sample_log = process_graphs(list(chains.values()), in_states, in_target_nodes, len(in_states))\n",
    "            \n",
    "        if states is not None:\n",
    "            logs.append({'sample': sample, 'target_nodes': target_nodes, 'log': sample_log})  # Add this ONE sample list to the main list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total: {total}\")\n",
    "print(f\"Solved logs length: {len(logs)}\")\n",
    "print(f\"Success rate: {round(len(logs)/total, 2)*100}%\")\n",
    "print(f\"Unsolved rate: {round((total-len(logs))/total,2)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "def parse_log(log):\n",
    "    dic = {}\n",
    "    init_state = log[1]\n",
    "    end_idx = init_state.index(\"]\")\n",
    "    init_state = init_state[2:end_idx+1]\n",
    "    # Parse the string into a Python list using ast.literal_eval\n",
    "    parsed_list = ast.literal_eval(init_state)\n",
    "    # Create a dictionary with indices as keys and list values as values\n",
    "    init_state_dict = {i: value for i, value in enumerate(parsed_list)}\n",
    "    # Print the result\n",
    "    dic['init_state'] = init_state_dict\n",
    "\n",
    "    # get all indexes of ***\n",
    "    start_idx = []\n",
    "    for i in range(len(log)):\n",
    "        if log[i].startswith(\"*\"*30):\n",
    "            start_idx.append(i+1)\n",
    "\n",
    "    target_nodes_lst = []\n",
    "    for tn in start_idx:\n",
    "        target_nodes_lst.append(int(log[tn][log[tn].index(\"TN\")+3:]))\n",
    "\n",
    "    # Parse the string into a Python list using ast.literal_eval\n",
    "    parsed_list = target_nodes_lst\n",
    "    # Create a dictionary with indices as keys and list values as values\n",
    "    parsed_target_states_dict = OrderedDict((i, value) for i, value in enumerate(parsed_list))\n",
    "\n",
    "    dic['stack'] = parsed_target_states_dict\n",
    "    \n",
    "    path = []\n",
    "    last_num_of_indents = 0\n",
    "    for item in log:\n",
    "\n",
    "        curr_num_of_indents = item.count(\"-\")\n",
    "\n",
    "        if \"RS\" in item:\n",
    "            path.append(item)\n",
    "            \n",
    "        if \" U\" in item or \" D\" in item:\n",
    "            path.append(item)\n",
    "\n",
    "        if \"RULE\" in item:\n",
    "            path.append(\"TF\")\n",
    "\n",
    "        if curr_num_of_indents < last_num_of_indents:\n",
    "            path.append(\"RL\")\n",
    "\n",
    "        \n",
    "\n",
    "        last_num_of_indents = curr_num_of_indents\n",
    "\n",
    "    processed_path = []\n",
    "    processed_path.append(\"TF\")\n",
    "    for i, item in enumerate(path):\n",
    "        if \"RS\" in item:\n",
    "            ruleset = ast.literal_eval(item.split(\"RS\")[1])\n",
    "            result_dict = {item['automata_id']: item['node_id'] for item in ruleset}\n",
    "\n",
    "            processed_path.append(f\"N {result_dict}\")\n",
    "            if len(result_dict) > 1:\n",
    "                print(\"more than 1\", result_dict)\n",
    "        \n",
    "        if \" U\" in item or \" D\" in item:\n",
    "            processed_path.append(item.replace(\"-\", \"\"))\n",
    "\n",
    "        if \"TF\" in item:\n",
    "            processed_path.append(\"TF\")\n",
    "\n",
    "        if \"RL\" in item:\n",
    "            processed_path.append(\"RL\")\n",
    "            processed_path.append(\"TF\")\n",
    "\n",
    "    processed_path.append(\"RL\")\n",
    "\n",
    "    # pprint(path)\n",
    "    # print(\"------------------------\")\n",
    "    # pprint(processed_path)\n",
    "\n",
    "    dic['path'] = processed_path\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# # load pkl file\n",
    "# with open(\"path_data.pkl\", \"rb\") as f:\n",
    "#     logs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs[0]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "i = 666\n",
    "print\n",
    "print(logs[i]['sample'])\n",
    "print(logs[i]['target_nodes'])\n",
    "pprint(logs[i]['log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_path = parse_log(logs[-1]['log'])\n",
    "print(dict_of_path['init_state'])\n",
    "print(dict_of_path['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_path = parse_log(logs[-1]['log'])\n",
    "print(dict_of_path['init_state'])\n",
    "print(dict_of_path['stack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for log in tqdm(logs):\n",
    "    dict_of_path = parse_log(log['log'])\n",
    "    paths.append(dict_of_path)\n",
    "\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save paths into pkl file\n",
    "import pickle\n",
    "with open(\"path_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(paths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths[-1]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pkl file\n",
    "import pickle\n",
    "with open(\"path_data.pkl\", \"rb\") as f:\n",
    "    paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    p = path['path']\n",
    "    for i in range(len(p)):\n",
    "        if \"N\" in p[i]:\n",
    "            print(p[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "int_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
